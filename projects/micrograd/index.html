<!doctype html><html><head><title>Machine Learning</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/google-fonts/Mulish/mulish.css><link rel=stylesheet href=/fontawesome/css/all.min.css><link rel=stylesheet href=/css/colortheme/colortheme.css><link rel=icon type=image/png href=/images/site/logo_no_bg_hu29c94b954602da118b51e4943b22ee93_110058_42x0_resize_box_3.png><meta property="og:title" content="Machine Learning"><meta property="og:description" content="These are my notes taken during implementing the micrograd library and training a neural net to recognise images.
Check back soon for a link to a working demo and code :)
What is micrograd? This is all based off Andrej Karpathy&rsquo;s Github repo and Youtube videos. I am a huge fan of him!
Micrograd is all you need to train neural networks. Everything else (pytorch/tensor flow) is just efficiency.
My notes We need to get the gradient of the loss function and step towards minimal value."><meta property="og:type" content="article"><meta property="og:url" content="https://kristian-94.github.io/projects/micrograd/"><meta property="article:section" content="projects"><meta name=description content="Machine Learning"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-H4LBG7NDFZ"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-H4LBG7NDFZ",{anonymize_ip:!1})}</script></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/><img src=/images/site/logo_white_hu499226ecff3a52db5ffa6bfbc5e1e8ae_94097_42x0_resize_box_3.png alt=Logo>
Kristian's Projects</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"><li class="nav-item dropdown"><div id=theme-initialization style=display:none default-theme=system></div><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme"></a><div class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# onclick=enableLightTheme()><img class=menu-icon-center src=/icons/sun-svgrepo-com.svg width=20 alt="Light Theme"></a>
<a class="dropdown-item nav-link" href=# onclick=enableDarkTheme()><img class=menu-icon-center src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme"></a>
<a class="dropdown-item nav-link" href=# onclick=useSystemTheme()><img class=menu-icon-center src=/icons/computer-svgrepo-com.svg width=20 alt="System Theme"></a></div></li></ul></div></div><img src=/images/site/logo_white_hu499226ecff3a52db5ffa6bfbc5e1e8ae_94097_42x0_resize_box_3.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/logo_black_hu3f95809e71e16daad4ce71c3d0baa67f_1283240_42x0_resize_box_3.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class="d-block content-section ml-auto mr-auto text-center" id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/micrograd_hero.svg);object-fit:scale-down></div><div class=page-content><div class="author-profile ml-auto mr-auto align-self-lg-center"><img class=rounded-circle src=/images/kristian-square_hu228dadcbca21be6860ae39ff7451bbd0_2080998_120x120_fit_box_3.png alt="Author Image"><h5 class=author-name></h5></div><div class=title><h1>Machine Learning</h1></div><div class=taxonomy-terms><ul style=padding-left:0></ul></div><div class="post-content text-left" id=post-content><p>These are my notes taken during implementing the micrograd library and training a neural net to recognise images.</p><p>Check back soon for a link to a working demo and code :)</p><h2 id=what-is-micrograd>What is micrograd?</h2><p>This is all based off <a href=https://en.wikipedia.org/wiki/Andrej_Karpathy>Andrej Karpathy</a>&rsquo;s Github repo and Youtube videos.
I am a huge fan of him!</p><p>Micrograd is all you need to train neural networks. Everything else (pytorch/tensor flow) is just efficiency.</p><div class=text-center><div class=github-card data-github=karpathy/micrograd data-width=400 data-height data-theme=default></div><script src=//cdn.jsdelivr.net/github-cards/latest/widget.js></script><br></div><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/VMj-3S1tku0 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h1 id=my-notes>My notes</h1><p>We need to get the gradient of the loss function and step towards minimal value. Change weights of inputs in a way that
will take us there.</p><p>Two files:
<code>nn.py</code> neural net library importing and using micrograd engine.
<code>engine.py</code> 100 lines of python code, no imports.</p><h3 id=nnpy>nn.py</h3><p><code>neuron</code> : one link in the chain of weights, ends up getting a value and connected to other neurons. The weight between
is the strength of that connection.
<code>layer</code> layer of neurons. They are all connected to the input but not eachother, a set of neurons evaluated
independently.
<code>MLP</code> multi layer perceptron: sequence of layers of neurons.</p><p>We are computing the derivative of the loss function with respect to the weights of the neural network.</p><h5 id=how-do-weights-affect-the-loss-function>How do weights affect the loss function?</h5><p>Use gradient information. Weights are leaf nodes, they are an input. The other leaf nodes are the data but that is
fixed.</p><h3 id=gradient-calculation>Gradient calculation</h3><p>In calculus, we can use the product rule to explicitly generate an equation to get the exact derivative.
However, we can&rsquo;t do that for these huge functions with 10k parameters and we don&rsquo;t need to.</p><h4 id=what-is-a-gradient>What is a gradient?</h4><p>A gradient is the slope of a graph at a point. We get this as a limit when increasing x by a small amount. Practically,
this means we just
need to insert a slightly larger number and rerun our function. Then we see how much it changed, positive or negative,
that&rsquo;s the gradient.</p><p>We normalise by dividing by the height, rise/run. <code>(L2 - L1)/h</code>
We can get the gradient of the final L loss with respect to any of the input elements, by only changing it, and then
seeing how much L changed. This is the way to verify the calculated overall gradient that is found by traversing each
node individually and applying the chain rule from the surrounding nodes.</p><p>If we know how much d affects L, and how much e affects d, we can then use the chain rule of derivatives to calculate
the overall derivative of e on L.</p><p>A <code>+</code> in the chart just pulls through the next node&rsquo;s gradient. A plus has a gradient of 1.0 with respect to the next
node, so it can just copy the next gradient to the children nodes. It&rsquo;s basically equivalent of a simple equation
eg. <code>y = x + c</code> -> gradient is 1.</p><h4 id=back-propagation>Back propagation</h4><p>This is a recursive application of the chain rule back through the computational graph.</p><h3 id=leaf-nodes>Leaf Nodes</h3><p>Some inputs are static data. The others are weights. Some others can be bias.</p><p>For each neuron:</p><ul><li>Take all the inputs and sum all the weights and values. (can have any number of inputs)</li><li>Add the bias.</li><li>Take it through an activation function (tanh/sigmoid). This squashes the input into a value between 0 - 1 which is the
final output and value of this neuron.</li></ul><p>We want the local derivative of the neuron&rsquo;s inputs on the final output. This whole thing is one little neuron in the
neural net. In the end we&rsquo;ll back propagate through the entire set of neurons for the overall loss function. We really
want the neurons output derived from the weights which is the thing we can change.</p></div></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://kristian-94.github.io#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://kristian-94.github.io#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=https://kristian-94.github.io#skills>Skills</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:kristian.ringer@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>kristian.ringer@gmail.com</span></a></li><li><a href=https://github.com/kristian-94 target=_blank rel=noopener><span><i class="fab fa-github"></i></span> <span>kristian-94</span></a></li><li><a href=https://www.linkedin.com/in/kristian-ringer-9ab515104 target=_blank rel=noopener><span><i class="fab fa-linkedin"></i></span> <span>Kristian Ringer</span></a></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hugo-toha/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">Â© 2025 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script>
<script type=text/javascript src=/js/popper.min.js></script>
<script type=text/javascript src=/js/bootstrap.min.js></script>
<script type=text/javascript src=/js/navbar.js></script>
<script type=text/javascript src=/js/plyr.js></script>
<script type=text/javascript src=/js/main.js></script>
<script type=text/javascript src=/js/darkreader.js></script>
<script type=text/javascript src=/js/darkmode-darkreader.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script>
<script src=/js/single.js></script>
<script>hljs.initHighlightingOnLoad()</script></body></html>